#!/bin/bash
#
# Instalador desatendido para Openstack Juno sobre UBUNTU 14.04lts
# Reynaldo R. Martinez P.
# E-Mail: TigerLinux@Gmail.com
# Octubre del 2014
#
# Archivo de configuracion principal
# Para el instalador versión 1.0.6.ub1404lts "Shadow Cat"
# Febrero 28, 2015
#

#
# Primero, variables de Base de Datos
#
# la siguiente variable define cual backend de Base de Datos se va a usar
# Hasta ahora sólo se soporta mysql y postgres
#
# dbflavor = mysql | postgres
dbflavor="mysql"

# La siguiente variable le indica al instalador si se creará la base de datos.
# Si la respuesta es "yes", se debe suministrar el usuario root y su password
# Se asume que no existe servidor de base de datos instalado y el instalador
# se encargará de crearlo SOLAMENTE si dbinstall = yes. En caso contrario, se
# asume que se tiene acceso a un servidor externo de base de datos
# dbcreate= yes | no
# dbinstall= yes | no
# dbcreate="yes" -> se crearán las bases de datos en el backend seleccionado y con
# los datos de acceso configurados mas abajo
# dbinstall="yes" -> se instalará el software de base de datos. Si usa esta opción en "yes",
# asegurese de colocar "dbcreate=yes" y mas adelante "dbpopulate=yes"
dbcreate="yes"
dbinstall="yes"
#
# Datos de acceso para BD MySQL
mysqldbadm="root"
mysqldbpassword="P@ssw0rd"
mysqldbport="3306"
#
# Datos de acceso para BD Postgres
psqldbadm="postgres"
psqldbpassword="P@ssw0rd"
psqldbport="5432"

# La siguiente variable le indica a los distintos componentes de Openstack
# (cinder, glance, keystone, etc.) que populen o no su base de datos.
# Si esta es una nueva instalación, la respuesta debe ser "yes", en caso
# contrario (reinstalación) use "no"
# dbpopulate = yes | no
dbpopulate="yes"
# NOTA: Si dbpopulate es "no", las opciones dbcreate y dbinstall no son tomadas en cuenta
# y no se toma ninguna acción para instalar, crear o poblar bases de datos.


# Aquí se define la IP del backend de base de datos:
# dbbackendhost = "IP o FQDN"
dbbackendhost="192.168.50.12"

# Si este es un controlador o un servidor monolítico, se debe colocar "yes"
# en el siguiente parámetro. Si es un nodo de compute que va a hablar con un qpidd remoto,
# coloque "no"
messagebrokerinstall="yes"
# Host de message broker
messagebrokerhost="192.168.50.12"
#
# Se incluye la posibilidad de seleccionar el backend de AMPQ de entre dos opciones.
# Por los momentos, qpid y rabbitmq
# La variable "brokerflavor" controla cual de los dos usar
# Adicionalmente, se activa la autenticación del message broker. Datos a continuación:
# BrokerFlavor: qpid o rabbitmq
# NOTA: Recomendamos ampliamente usar RabbitMQ
brokerflavor="rabbitmq"
# Datos de autenticación
brokeruser="openstack"
brokerpass="P@ssw0rd"
# Host virtual - sólo para RabbitMQ - se recomienda dejar "/openstack"
# ALERTA: Siempre incluir el "/" en el nombre de Host Virtual
brokervhost="/openstack"

# Ahora se definien los datos de acceso, nombre, etc. de las bases de datos
# de cada componente - los nombres deberían ser autoexplicativos
#
# Keystone:
keystonedbname="keystonedb"
keystonedbuser="keystonedbuser"
keystonedbpass="P@ssw0rd"
# Glance
glancedbname="glancedb"
glancedbuser="glancedbuser"
glancedbpass="P@ssw0rd"
# Cinder
cinderdbname="cinderdb"
cinderdbuser="cinderdbuser"
cinderdbpass="P@ssw0rd"
# Neutron
neutrondbname="neutrondb"
neutrondbuser="neutrondbuser"
neutrondbpass="P@ssw0rd"
# Nova
novadbname="novadb"
novadbuser="novadbuser"
novadbpass="P@ssw0rd"
# Horizon
horizondbname="horizondb"
horizondbuser="horizondbuser"
horizondbpass="P@ssw0rd"
# Heat
heatdbname="heatdb"
heatdbuser="heatdbuser"
heatdbpass="P@ssw0rd"
# Trove
trovedbname="trovedb"
trovedbuser="trovedbuser"
trovedbpass="P@ssw0rd"
# Sahara
saharadbname="saharadb"
saharadbuser="saharadbuser"
saharadbpass="P@ssw0rd"

# En la siguiente seccion se definen los datos de keystone
# Si se requiere instalar keystone, dejar keystoneinstall="yes",
# caso contrario, colocar "no" (caso típico de un compute)
keystoneinstall="yes"
keystonehost="192.168.50.12"
# Solo para la base de datos:
# Si se van a tener varios hosts de keystone, colocar aquí las IP's
# para el acceso de la base de datos - sólo efectivo cuando el backend
# es MySQL
extrakeystonehosts=""
# Regenere el siguiente valor con el comando "openssl rand -hex 10"
SERVICE_TOKEN="044cb8c9e4a00a418827"
keystoneadminuser="admin"
keystoneservicename="keystone"
keystoneadminuseremail="admin@localhost"
keystoneadminpass="P@ssw0rd"
keystoneadmintenant="admin"
keystoneservicestenant="services"
keystonememberrole="Member"
keystonereselleradminrole="ResellerAdmin"
keystone_admin_rc_file="/root/keystonerc_admin"

# Flavor para el tipo de proveedor de TOKEN para Keystone
# Dos opciones: pki o uuid
# Por defecto: uuid
keystonetokenflavor="uuid"

# Region para todos los endpoints
# Ejemplo: MiSitioDeOpenStack
endpointsregion="RegionOne"

#
# Extra Tenants - los usuarios creados inicialmente para estos
# tenants extras tendrán el mismo nombre del tenant y el password
# configurado inicialmente para cada tenant sera el base + "-" + tenant
# ejemplo:
# extratenantbasepass=P@ssw0rd
# nombre del tenant = extra1
# El password sería: extra1-P@ssw0rd
extratenantbasepass="P@ssw0rd"
extratenants="extra1 extra2 extra3"
domainextratenants="localhost"
# NOTA: Estos tenants/usuarios extras se agregarán al role Member
# El dominio será el mismo para todos, pero luego se puede cambiar
# en la interfaz web desde el tenant/usuario administrativo



# Ahora los datos de Swift. Igual que en el caso anterior, se puede
# elegir o no instalar el componente
# NOTA: Recomendación - No Usarlo para una instalación estandar de OpenStack
# Switf requiere un nivel alto de programación e interacción con su API. En
# instalaciones normales, no vale la pena instalarlo. Default: NO
swiftinstall="no"
swifthost="192.168.50.12"
swiftuser="swift"
swiftsvce="swift"
swiftpass="P@ssw0rd"
swiftemail="swift@localhost"
#
# Datos extras de configuracion para swift
#
# NOTA: partition count debe ser 2^partition_power
# y a su vez partition_count debe ser de 100 en adelante
# Para este "default", 2^7 = 128
# Vea la información en los manuales de swift
partition_power="7"
partition_count="128"
partition_min_hours="10"
replica_count="5"
swiftfirstzone="1"
#
# Nota. Este device es el nombre del punto de montaje dentro de /srv/node. Este
# punto debe estar montado en /etc/fstab de la siguiente manera:
# /dev/sdc1               /srv/node/d1            ext4    acl,user_xattr  0 0
# El punto de montaje DEBE ESTAR DISPONIBLE o swift no será instalado
swiftdevice="d1"
# Si la siguiente variable es "yes", durante la instalación de swift cualquier
# contenido en el device de swift especificado en la variable anterior será
# eliminado.. específicamente los archivos de swift
# USAR CON EXTREMA PRECAUCION !!
cleanupdeviceatinstall="no"
# Mismo caso pero aplica en el uninstall
cleanupdeviceatuninstall="yes"
#
# Si el siguiente valor es "yes", se reconfigurará glance para usar
# swift como backend de almacenamiento
glance_use_swift="no"
#



# Ahora los datos de glance. Igual que en el caso anterior, se puede
# elegir o no instalar glance
glanceinstall="yes"
glancehost="192.168.50.12"
glanceuser="glance"
glancesvce="glance"
glancepass="P@ssw0rd"
glanceemail="glance@localhost"
#
# Alternativamente, podemos crear de una vez las imágenes de prueba
# para Cirros
# Dejando la siguiente opción en "yes", el instalador dejará las imágenes
# de prueba de cirros 0.3.1 32 y 64 bits ya creadas y listas para ser
# utilizadas
glancecirroscreate="yes"
# Solo para la base de datos:
# Si se van a tener varios hosts de glance, colocar aquí las IP's
# para el acceso de la base de datos - sólo efectivo cuando el backend
# es MySQL
extraglancehosts=""



# Mismo caso para cinder:
cinderinstall="yes"
cinderhost="192.168.50.12"
cinderuser="cinder"
cindersvce="cinder"
cindersvcev2="cinderv2"
cinderpass="P@ssw0rd"
cinderemail="cinder@localhost"
cinder_iscsi_ip_address="192.168.50.12"
# Solo para la base de datos:
# Si se van a tener varios hosts de cinder, colocar aquí las IP's
# para el acceso de la base de datos - sólo efectivo cuando el backend
# es MySQL
extracinderhosts=""



# Igual para neutron
# Y se definen las redes de neutron (br-int, br-ethx, red publica inicial, etc.)
neutroninstall="yes"
#
# Si el nodo que está usted instalando es un nodo Neutron Completo (ejemplo, en
# un controller o un nodo de red) la siguiente variable debe estar en "no". Si en
# cambio usted está instalando un nodo de "Nova Compute", la variable debe ir en
# "yes" para incluir sólamente el soporte del agente de OpenVSwitch para Neutron
# en la instalación del nova-compute.
# En resumen:
# Controller, All-In-One o Nodo de Red: "no"
# Nodo de Nova Compute: "yes"
neutron_in_compute_node="no"
#
#
neutronhost="192.168.50.12"
#
# Si está usando neutron en un nodo de COMPUTE, coloque
# la IP del nodo de compute en la siguiente variable. Si
# se trata de un ALL-IN-ONE o un nodo de neutron (neutron-server), coloque
# la misma IP que en la variable neutronhost:
neutron_computehost="192.168.50.12"
#
#
neutronuser="neutron"
neutronsvce="neutron"
neutronpass="P@ssw0rd"
neutronemail="neutron@localhost"
network_vlan_ranges="publica"
integration_bridge="br-int"
bridge_mappings="publica:br-eth2"
dhcp_domain="localdomain"
metadata_shared_secret="P@ssw0rd"
# MAC base para las VM's
basemacspec="fa:16:3e:00:00:00"
# Duración en segundos para los leases
dhcp_lease_duration="7200"
# Solo para la base de datos:
# Si se van a tener varios hosts de neutron, colocar aquí las IP's
# para el acceso de la base de datos - sólo efectivo cuando el backend
# es MySQL
extraneutronhosts=""
#
# NOTA: El módulo de instalación de Neutron tomará
# el archivo original de configuración de dnsmasq
# y lo copiará al indicado en la variable siguiente
# Puede cambiarle el nombre al que desee, siempre y cuando
# no sea "/etc/dnsmasq.conf" que es el archivo original
# En este archivo, ustedes puede hacer configuraciones
# personalizadas para ser enviadas al DHCP de las VM's
dnsmasq_config_file="/etc/dnsmasq-neutron.conf"
#
# NOTA IMPORTANTE
# La instalación dejará el siguiente archivo para colocar opciones
# adicionales para el DNSMASQ de Neutron
# /etc/dnsmasq-neutron.d/neutron-dnsmasq-extra.conf
#
# Auto creacion de redes en neutron:
# Si colocar el valor de network_create en "yes"
# debe colocar una lista de redes a crear. Ejemplo
# network_create_list="publica privada externa interna"
network_create="yes"
network_create_list="publica"
#
# Nuevo para HAVANA: VPNaaS (VPN as a Service)
# Por defecto, no lo instalamos - coloque "yes" si lo quiere instalar
vpnaasinstall="no"
#
# Si se va a instalar el módulo de meetering (solo para esquemas que
# usen routers L2 de Neutron), colocar la siguiente variable en "yes"
neutronmetering="yes"



# Datos de configuracion para nova
novainstall="yes"
#
# Si el nodo que está usted instalando es un nodo Nova Completo (ejemplo, en
# un controller) la siguiente variable debe estar en "no". Si en
# cambio usted está instalando un nodo de "Nova Compute", la variable debe ir en
# "yes" para incluir sólamente el soporte mínimo de Nova Compute.
# En resumen:
# Controller o All-In-One: "no"
# Nodo de Nova Compute: "yes"
nova_in_compute_node="no"
# 
# Adicionalmente, si va a instalar un controller sin servicio de compute, coloque
# la siguiente variable en "yes" y el servicio de nova-compute no iniciará en el
# servidor. Caso contrario, déjela como está en "no".
#
nova_without_compute="no"
#
#
novahost="192.168.50.12"
#
# Si está usando NOVA en un nodo de COMPUTE, coloque
# la IP del nodo de compute en la siguiente variable. Si
# se trata de un ALL-IN-ONE o un nodo de NOVA (nova server), coloque
# la misma IP que en la variable novahost:
nova_computehost="192.168.50.12"
novauser="nova"
novasvce="nova"
novaec2svce="ec2"
novapass="P@ssw0rd"
novaemail="nova@localhost"
# Solo para la base de datos:
# Si se van a tener varios hosts de nova, colocar aquí las IP's
# para el acceso de la base de datos - sólo efectivo cuando el backend
# es MySQL
extranovahosts=""
#
# Datos para sobre-suscripción de CPU y RAM en los nodos de compute
ram_allocation_ratio="1.5"
cpu_allocation_ratio="16.0"
# A partir del instalador para Ubuntu, damos la opción de instalar VNC o Spice
# La siguiente variable controla que sabor de consola se va a usar.
# En el instalador para Centos se usa el mismo principio.
# Coloque "spice" para SpiceHTML5, o "vnc" para NoVNC
# Por defecto: vnc
consoleflavor="vnc"
# Servidor SPICE
# Esta es la IP que debe pertenecer al controller o nodo NOVA de control
# En una configuración all-in-one, esa la misma del novahost, pero en una
# configuración con múltiples nodos de compute, debe ser la IP del controller
# o donde se ejecuta el nodo principal de NOVA
spiceserver_controller_address="192.168.50.12"
# Servidor VNC
# Esta es la IP que debe pertenecer al controller o nodo NOVA de control
# En una configuración all-in-one, esa la misma del novahost, pero en una
# configuración con múltiples nodos de compute, debe ser la IP del controller
# o donde se ejecuta el nodo principal de NOVA
vncserver_controller_address="192.168.50.12"
# Keymap: Valores válidos: us y es
vnc_keymap="es"
#
# Si el valor siguiente es yes, Nova creará
# los grupos de seguridad por defecto para dar acceso
# a las VM's por icmp y ssh
vm_default_access="yes"
# En la siguiente variable se crean accesos adicionales
# a las VM's
vm_extra_ports_tcp="25 53 80 110 143 443"
vm_extra_ports_udp="53"
#
# Deje esta opción en "True" para nodos de compute individuales
# y en "False" si su instalación tiene múltiples nodos de compute
# trabajando en conjunto
allow_resize_to_same_host="True"
#
# Las siguientes opciones le indican a nova
# Si va a levantar las VM's en el arranque/reboot y/o si
# va a levantar las VM's en su último estado luego de un reboot
# del compute. Valores posibles: True o False. Por defecto: False:
start_guests_on_host_boot="False"
resume_guests_state_on_host_boot="False"
#
# Nombre del template para las VM's
# Esto sólo afecta el nombre interno de las VM's:
instance_name_template="instancia-%08x"
#
# Modo de CPU para el compute - sólo válido para kvm
# Para qemu este valor no se usará.
# Se recomienda (para kvm) dejar el valor host-passthrough
# Mas información: https://wiki.openstack.org/wiki/LibvirtXMLCPUModel
# Valores mas comunes: host-passtrough o host-model
libvirt_cpu_mode="host-passthrough"



# Datos de configuración para ceilometer
#
ceilometerinstall="yes"
# Si el nodo que está usted instalando es un nodo Ceilometer Completo (ejemplo, en
# un controller o un nodo de ceilometer) la siguiente variable debe estar en "no".
# Si en cambio usted está instalando un nodo de "Compute" (con Nova), la variable 
# debe ir en "yes" para incluir sólamente el soporte del agente de ceilometer para
# recolectar datos de Nova Compute.
# En resumen:
# Controller, All-In-One o Nodo de Ceilometer: "no"
# Nodo de Compute con agente de ceilometer: "yes"
ceilometer_in_compute_node="no"
#
#
# Adicionalmente, si va a instalar un controller o nodo de ceilometer sin servicio
# de compute, coloque la siguiente variable en "yes" y el servicio de recolección de
# datos de ceilometer para compute no iniciará en el servidor. Caso contrario, déjela 
# como está en "no".
#
ceilometer_without_compute="no"
#
ceilometerhost="192.168.50.12"
ceilometeruser="ceilometer"
ceilometersvce="ceilometer"
ceilometerpass="P@ssw0rd"
ceilometeremail="ceilometer@localhost"
# Datos de acceso de la BD "mongodb" de Ceilometer
# Se recomienda ALTAMENTE no modificar estos valores
# a excepción de la dirección IP del host
mondbhost="192.168.50.12"
mondbport="27017"
mondbname="ceilometer"
mondbuser="ceilometer"
# NOTA: Por alguna razon, el password para MongoDB no puede contener
# arrobas u otros caracteres que requieran ser "escapados"
# Use un pass con ascii básico y números.. Please !!
mondbpass="C3iL0m3T3r-Op3nStaCk"
# Password del servicio de metering
# Regenerarlo con el comando: openssl rand -hex 10
metering_secret="fe01a6ed3e04c4be1cd8"
#
# Si se va a instalar el módulo de alarmas, colocar
# la siguiente variable en "yes"
ceilometeralarms="yes"




# Datos de configuración para heat
# Por defecto va en "no" a menos que lo vaya a
# usar realmente
heatinstall="no"
heathost="192.168.50.12"
heatuser="heat"
heatsvce="heat"
heatpass="P@ssw0rd"
heatemail="heat@localhost"
heatcfnsvce="heat-cfn"
# Regenere el siguiente valor con el comando "openssl rand -hex 10"
heatencriptionkey="e0dd8525f037e1b8cb6e"
# Solo para la base de datos:
# Si se van a tener varios hosts de nova, colocar aquí las IP's
# para el acceso de la base de datos - sólo efectivo cuando el backend
# es MySQL
extraheathosts=""



##########
## NOTA ##
##########

#
# Los paquetes de TROVE-JUNO aun no están disponibles en el repositorio de
# Ubuntu, pero se deja lo necesario a nivel de configuración para cuando
# están listos para Ubuntu.
#


# Datos de configuración para trove
# Por defecto va en "no" a menos que lo vaya a
# usar realmente
#
troveinstall="no"
trovehost="192.168.50.12"
troveuser="trove"
trovesvce="trove"
trovepass="P@ssw0rd"
troveemail="trove@localhost"
# Solo para la base de datos:
# Si se van a tener varios hosts de nova, colocar aquí las IP's
# para el acceso de la base de datos - sólo efectivo cuando el backend
# es MySQL
extratrovehosts=""


##########
## NOTA ##
##########

#
# Los paquetes de SAHARA-JUNO aun no están disponibles en el repositorio de
# Ubuntu, pero se deja lo necesario a nivel de configuración para cuando
# están listos para Ubuntu.
#

# Datos de configuración para sahara
# Por defecto va en "no" a menos que lo vaya a
# usar realmente
#
saharainstall="no"
saharahost="192.168.50.12"
saharauser="sahara"
saharasvce="sahara"
saharapass="P@ssw0rd"
saharaemail="sahara@localhost"
# Solo para la base de datos:
# Si se van a tener varios hosts de nova, colocar aquí las IP's
# para el acceso de la base de datos - sólo efectivo cuando el backend
# es MySQL
extrasaharahosts=""


# Instalacion de infraestructura de monitoreo
# Si quiere monitorear su OpenStack, lo puede hacer
# via snmp - por defecto: no
# Si coloca "yes" y ya tiene un archivo de "snmp"
# configurado, se le agregarán las nuevas variables
# Si no tiene nada configurado, se creará un nuevo
# archivo snmpd con la community RO "OpenStack", con la "O"
# y la "S" en mayúsculas.
# Cambie la comunidad en /etc/snmp/snmpd.conf segun
# sus necesidades
snmpinstall="no"


# Horizon
# NOTA: Los "slashes" deben ir escapados
# Normalmente, estos valores no deberían ser cambiados
# (como mucho horizoninstall="no")
horizoninstall="yes"
# Si va a usar una base de datos con Horizon, coloque "yes"
# Por defecto lo hacemos - ALTAMENTE Recomendable
horizondbusage="yes"
dashboard_timezone="America\/Caracas"
horizonhost="192.168.50.12"
# Solo para la base de datos:
# Si se van a tener varios hosts de horizon, colocar aquí las IP's
# para el acceso de la base de datos - sólo efectivo cuando el backend
# es MySQL
extrahorizonhosts=""
memcached_spec="memcached:\/\/127.0.0.1:11211\/"
brandingname="OpenStack Private Cloud"

#
# FIN del archivo de configuración
#
